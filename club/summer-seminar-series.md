---
title: Summer Seminar Series
header:
  title: Summer Seminar Series
---
<!-- Image styling -->
<style>
img.speaker {
  width: 200px;
  height: 286px;
  object-fit: cover;
}
</style>

<!-- The table of speakers -->
<div style="margin-bottom: 1rem">
  <div class="row" style="margin-bottom: 1rem">
    <div class="col-lg-3">
      <img src="/assets/img/club_photos/n_gupta.png" alt="Picture of Nitish Gupta" class="rounded shadow speaker">
    </div>
    <div class="col-lg-9">
        <h4><a href="https://nitishgupta.github.io/" target="_blank">Nitish Gupta</a></h4>
        <h6>Presenting May 14, 2020 @ 12 PM MDT</h6>
        <h6>Zoom Link: <a href="https://us02web.zoom.us/j/89639806093" target="_blank" style="text-decoration:underline;">https://us02web.zoom.us/j/89639806093</a></h6>
        <h6>Title: Neural Module Networks for Reasoning over Text</h6>
        <p>Nitish is a senior PhD student in Computer Science at the University of Pennsylvania pursuing research in NLP. He is advised by Prof. Dan Roth and co-advised by Prof. Sameer Singh, UCI. Nitish's research focuses on developing structured models for grounded language understanding, primarily in the context of making machines understand and answer questions against text. He is particularly interested in models that perform reasoning by understanding the compositional nature of language, and are able to provide a formal semantic parse and an explanation about their predictions. For more details you can visit <a href="https://nitishgupta.github.io/" target="_blank">https://nitishgupta.github.io/</a>.</p>
    </div>
  </div>
</div>

*Abstract:* Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations. In this talk, I will outline the challenges in learning these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broad range of reasoning. I will present our interpretable and modular neuro-symbolic approach based on neural module networks (NMNs) and show how we extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning over numbers and dates in a probabilistic and differentiable manner; and (b) proposing unsupervised auxiliary loss to help learning. In conclusion, I will present our recent work on methods for achieving faithfull interpretations in such compositional models and challenges for future research.

<br>
<center><h1>Next Speaker TBD</h1></center>